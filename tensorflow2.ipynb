{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH2JmKaKukPR"
      },
      "source": [
        "When dealing with datasets of Images in zip format.\n",
        "\n",
        "Step 1:\n",
        "=====\n",
        "\t1)create a zip folder locally.\n",
        "\n",
        "\t2)Reference to the folder where the folder is present\n",
        "\n",
        "\t3)extract the files in folder by using the reference folder\n",
        "\n",
        "\t4)close the zip folder\n",
        "\n",
        "\n",
        "Interms of python code:\n",
        "\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# Unzip the dataset\n",
        "local_zip = './horse-or-human.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('./horse-or-human')\n",
        "\n",
        "zip_ref.close()\n",
        "\n",
        "\n",
        "Step 2:\n",
        "=====\n",
        "\n",
        "How to categorize the data\n",
        "\n",
        "\t1)from the base folder create a sub folder.\n",
        "\n",
        "Interms of code:\n",
        "\n",
        "import os\n",
        "\n",
        "# Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "train_human_dir = os.path.join('./horse-or-human/humans')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYs5M-dVwu4l"
      },
      "source": [
        "**Downloaading image dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw4h3lpxunk7",
        "outputId": "ec0133f0-f818-4598-de8c-daaf55e01b92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-06-08 17:16:01--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.207, 108.177.97.207, 108.177.125.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘cats_and_dogs_filtered.zip’\n",
            "\n",
            "cats_and_dogs_filte 100%[===================>]  65.43M  19.0MB/s    in 3.6s    \n",
            "\n",
            "2024-06-08 17:16:06 (18.1 MB/s) - ‘cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQQxMbJbxUzD"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzip the archive\n",
        "local_zip = './cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall()\n",
        "\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFkEWsWUx97M"
      },
      "source": [
        "This code snippet imports the os module and defines a variable base_dir pointing to the directory containing the dataset.\n",
        "\n",
        "Next, it prints the contents of the base directory, the train subdirectory, and the validation subdirectory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiLXnRA4xX4-",
        "outputId": "1bf4d097-12f5-442c-96aa-ba29ac1721a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of base directory:\n",
            "['validation', 'vectorize.py', 'train']\n",
            "\n",
            "Contents of train directory:\n",
            "['cats', 'dogs']\n",
            "\n",
            "Contents of validation directory:\n",
            "['cats', 'dogs']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "base_dir = 'cats_and_dogs_filtered'\n",
        "\n",
        "print(\"Contents of base directory:\")\n",
        "print(os.listdir(base_dir))\n",
        "\n",
        "print(\"\\nContents of train directory:\")\n",
        "print(os.listdir(f'{base_dir}/train'))\n",
        "\n",
        "print(\"\\nContents of validation directory:\")\n",
        "print(os.listdir(f'{base_dir}/validation'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oycKnBAtzBIm"
      },
      "source": [
        "This code snippet defines several new variables containing paths to different subdirectories within the dataset directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyBU28FHx2Ng"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with training cat/dog pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with validation cat/dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex2P0BiO70Cr"
      },
      "source": [
        "**The main purpose of the activation function is to introduce the property of nonlinearity into the model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo2RGMFuzM39"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE6HpyDZzRhG",
        "outputId": "7c6c81d9-db4b-4ac0-b06a-79adfd69315d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 74, 74, 16)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 36, 36, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 17, 17, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18496)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               9470464   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9494561 (36.22 MB)\n",
            "Trainable params: 9494561 (36.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba0sSfSmzVTJ"
      },
      "source": [
        "optimization and loss function\n",
        "\n",
        "we are defing the learning ratefor optimizer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUQ0j0R3zXyr"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-aNeWM4zgNx"
      },
      "source": [
        "**Data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S0-LdDSxGcn",
        "outputId": "16909e9a-57be-4a09-8923-40d9700afb42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255.\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# --------------------\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "# --------------------\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))\n",
        "# --------------------\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "# --------------------\n",
        "validation_generator =  test_datagen.flow_from_directory(validation_dir,\n",
        "                                                         batch_size=20,\n",
        "                                                         class_mode  = 'binary',\n",
        "                                                         target_size = (150, 150))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb3SB-cK9ys7"
      },
      "source": [
        "By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch.\n",
        "\n",
        "verbose=0 will show you nothing (silent)\n",
        "\n",
        "verbose=1 will show you an animated progress bar like this:\n",
        "         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXuyOX6uxHyh",
        "outputId": "d140762f-6970-4d4c-cab0-fad73ef40302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "100/100 - 63s - loss: 0.6983 - accuracy: 0.5695 - val_loss: 0.6429 - val_accuracy: 0.6440 - 63s/epoch - 627ms/step\n",
            "Epoch 2/15\n",
            "100/100 - 58s - loss: 0.6276 - accuracy: 0.6450 - val_loss: 0.5782 - val_accuracy: 0.6850 - 58s/epoch - 577ms/step\n",
            "Epoch 3/15\n",
            "100/100 - 58s - loss: 0.5472 - accuracy: 0.7310 - val_loss: 0.5407 - val_accuracy: 0.7320 - 58s/epoch - 583ms/step\n",
            "Epoch 4/15\n",
            "100/100 - 63s - loss: 0.4890 - accuracy: 0.7625 - val_loss: 0.5338 - val_accuracy: 0.7570 - 63s/epoch - 627ms/step\n",
            "Epoch 5/15\n",
            "100/100 - 56s - loss: 0.4510 - accuracy: 0.7870 - val_loss: 0.5192 - val_accuracy: 0.7380 - 56s/epoch - 563ms/step\n",
            "Epoch 6/15\n",
            "100/100 - 56s - loss: 0.4006 - accuracy: 0.8265 - val_loss: 0.5379 - val_accuracy: 0.7330 - 56s/epoch - 556ms/step\n",
            "Epoch 7/15\n",
            "100/100 - 62s - loss: 0.3310 - accuracy: 0.8545 - val_loss: 0.5950 - val_accuracy: 0.7320 - 62s/epoch - 617ms/step\n",
            "Epoch 8/15\n",
            "100/100 - 55s - loss: 0.2633 - accuracy: 0.8875 - val_loss: 0.6444 - val_accuracy: 0.7360 - 55s/epoch - 553ms/step\n",
            "Epoch 9/15\n",
            "100/100 - 62s - loss: 0.2006 - accuracy: 0.9230 - val_loss: 0.6572 - val_accuracy: 0.7520 - 62s/epoch - 617ms/step\n",
            "Epoch 10/15\n",
            "100/100 - 58s - loss: 0.1434 - accuracy: 0.9500 - val_loss: 0.8038 - val_accuracy: 0.7290 - 58s/epoch - 582ms/step\n",
            "Epoch 11/15\n",
            "100/100 - 56s - loss: 0.1042 - accuracy: 0.9715 - val_loss: 0.9451 - val_accuracy: 0.7370 - 56s/epoch - 564ms/step\n",
            "Epoch 12/15\n",
            "100/100 - 57s - loss: 0.0516 - accuracy: 0.9865 - val_loss: 1.1592 - val_accuracy: 0.7400 - 57s/epoch - 568ms/step\n",
            "Epoch 13/15\n",
            "100/100 - 58s - loss: 0.0582 - accuracy: 0.9845 - val_loss: 1.0511 - val_accuracy: 0.7330 - 58s/epoch - 577ms/step\n",
            "Epoch 14/15\n",
            "100/100 - 56s - loss: 0.0265 - accuracy: 0.9900 - val_loss: 1.3284 - val_accuracy: 0.7510 - 56s/epoch - 563ms/step\n",
            "Epoch 15/15\n",
            "100/100 - 57s - loss: 0.0277 - accuracy: 0.9935 - val_loss: 1.3615 - val_accuracy: 0.7560 - 57s/epoch - 567ms/step\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "            train_generator,\n",
        "            epochs=15,\n",
        "            validation_data=validation_generator,\n",
        "            verbose=2\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "rpACqxoAz0jq",
        "outputId": "562053f0-eda6-4176-e5cb-7e3d8199751f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5dd9f0b6-f07e-4a84-aba3-958c046b8eb6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5dd9f0b6-f07e-4a84-aba3-958c046b8eb6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "uploaded=files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "  # predicting images\n",
        "  path='/content/' + fn\n",
        "  img=load_img(path, target_size=(150, 150))\n",
        "\n",
        "  x=img_to_array(img)\n",
        "  x /= 255\n",
        "  x=np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "\n",
        "  print(classes[0])\n",
        "\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a dog\")\n",
        "  else:\n",
        "    print(fn + \" is a cat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LxnCY_23N7S"
      },
      "source": [
        "**Data Augmentation and Image Augmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYG7agT23t0E"
      },
      "source": [
        "Image Augmenation or data Augmenation  means adding the elements to the  training dataset with Augmentation techniques like rotating the images and croping the images.\n",
        "\n",
        "This is done in the memory(ram) using the tensorflow\n",
        "\n",
        "  1)Image Generator\n",
        "\n",
        "  2)Image DataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLABbgzm5e4S"
      },
      "source": [
        "You'll be looking a lot at Image Augmentation this week.\n",
        "\n",
        "Image Augmentation is a very simple, but very powerful tool to help you avoid overfitting your data. The concept is very simple though: If you have limited data, then the chances of you having data to match potential future predictions is also limited, and logically, the less data you have, the less chance you have of getting accurate predictions for data that your model hasn't yet seen. To put it simply, if you are training a model to spot cats, and your model has never seen what a cat looks like when lying down, it might not recognize that in future.\n",
        "\n",
        "Augmentation simply amends your images on-the-fly while training using transforms like rotation. So, it could 'simulate' an image of a cat lying down by rotating a 'standing' cat by 90 degrees. As such you get a cheap way of extending your dataset beyond what you have already.\n",
        "\n",
        "To learn more about Augmentation, and the available transforms, check out\n",
        "https://keras.io/api/layers/preprocessing_layers/\n",
        " -- and note that it's referred to as preprocessing for a very powerful reason: that it doesn't require you to edit your raw images, nor does it amend them for you on-disk. It does it in-memory as it's performing the training, allowing you to experiment without impacting your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iimuz4gh3SQ7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}